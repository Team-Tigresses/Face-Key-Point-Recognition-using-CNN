# -*- coding: utf-8 -*-
"""FKR using CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WfgDJ4DyWQ5CGQXzeGfgSCuLtiB-4K5Z
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import os
import zipfile

# Set file paths:This code sets the paths for the zip files containing the training and testing data, the file containing the lookup table for the output, and a sample submission file.
train_zip_path = "/content/training.zip"
test_zip_path = "/content/test.zip"
id_lookup_table_path = "/content/IdLookupTable.csv"
sample_submission_path = "/content/SampleSubmission.csv"
# Extract data from zip files into separate folders named "train_data" and "test_data".
with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:
    zip_ref.extractall('train_data')
with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:
    zip_ref.extractall('test_data')

# Load data into dataframes: This code reads the CSV files in the "train_data" and "test_data" folders into Pandas dataframes, and reads the lookup table and sample submission files into dataframes as well.
train_df = pd.read_csv('train_data/training.csv')
test_df = pd.read_csv('test_data/test.csv')
id_lookup_table = pd.read_csv(id_lookup_table_path)
sample_submission = pd.read_csv(sample_submission_path)

# Print information about train_df
print(train_df.info())

#Fill the Nan values and Separate and reshape input values(x_train): This code fills the missing values in the "train_df" dataframe using forward fill (i.e., copying the last valid value to fill the missing values).

train_df.fillna(method='ffill',inplace=True)

#Separate and reshape input values(x_train) in the training dataset :
#This code extracts the "Image" column from the "train_df" dataframe into a separate dataframe called "image_df", then converts the pixel values into an array of floats. The array is then reshaped into a 4-dimensional array of size (number of examples, 96, 96, 1), where the last dimension corresponds to the number of channels (in this case, 1 since the images are grayscale). The resulting array is stored in "x_train", and its shape is printed.
image_df = train_df['Image']
imageArr = []
for i in range(0,len(image_df)):
     img = image_df[i].split()
     img = ['0' if x == '' else x for x in img]
     imageArr.append(img)
#The pixel values for each image are stored in the imageArr list, and they are converted to a numpy array and reshaped to be a 4-dimensional array with shape (number of images, height, width, channels). The channels dimension is set to 1 because the images are grayscale. The shape of the resulting x_train array is printed.
x_train = np.array(imageArr,dtype='float')
x_train = x_train.reshape(-1,96,96,1)
print(x_train.shape)

#Create a CNN that takes pictures as input and outputs key points: This code creates a new dataframe called "keypoints_df" by dropping the "Image" column from the "train_df" dataframe. The remaining columns correspond to the key point coordinates. These coordinates are converted to an array of floats and stored in "y_train". The shape of this array is printed.
keypoints_df = train_df.drop('Image',axis = 1)
y_train = np.array(keypoints_df,dtype='float')
print(y_train.shape)

#This function takes an index as input and displays the corresponding image from the x_train array using matplotlib's imshow() method. The image is reshaped to its original dimensions, and the colormap is set to grayscale. The function then plots red circles at the key point coordinates from the corresponding row of the y_train array.
def visualizeWithNoKeypoints(index):
    plt.imshow(x_train[index].reshape(96,96),cmap='gray')
def visualizeWithKeypoints(index):
    plt.imshow(x_train[index].reshape(96,96),cmap='gray')
    for i in range(1,31,2):
        plt.plot(y_train[0][i-1],y_train[0][i],'ro')

#After we have written the visualize function and next, we can visualize each image using the function call
#This code creates a figure that contains two subplots, one for each visualization function defined above. The first subplot displays the image without keypoints and the second subplot displays the image with keypoints.

import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8,4))
fig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.05)
plt.subplot(1,2,1)
visualizeWithNoKeypoints(1)
plt.subplot(1,2,2)
visualizeWithKeypoints(1)

#The data has been preprocessed. To create our CNN model, weâ€™ll utilize the Keras framework.

import keras
import tensorflow as tf

print(keras.__version__)
print(tf.__version__)

from keras.models import Sequential, Model
from keras.layers import Activation, Convolution2D,MaxPooling2D,BatchNormalization, Flatten, Dense, Dropout
from keras.layers import LeakyReLU

from keras.models import Sequential, Model
from keras.layers import Activation, Convolution2D,MaxPooling2D,BatchNormalization, Flatten, Dense, Dropout

model = Sequential()

model.add(Convolution2D(32,(3,3),padding='same',use_bias=False, input_shape=(96,96,1)))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(Convolution2D(32,(3,3),padding='same',use_bias = False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Convolution2D(64,(3,3),padding='same',use_bias = False))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())

model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))

model.add(BatchNormalization())

model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))
model.add(LeakyReLU(alpha = 0.1))
model.add(BatchNormalization())

model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))

model.add(BatchNormalization())
model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))
model.add(LeakyReLU(alpha = 0.1))

model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(30))
model.summary()

# import tensorflow as tf
# import os
# import pandas as pd
# import numpy as np
# from dataset import get_data
# from utils import evaluation_keypoints_plot, test_keypoints_plot
# from tqdm import tqdm
# # Create directory to save validation results.
# validation_result_path = os.path.join('..', 'outputs', 'validation_results')
# os.makedirs(os.path.join(validation_result_path), exist_ok=True)
# # Create directory to save test results.
# test_result_path = os.path.join('..', 'outputs', 'test_results')
# os.makedirs(os.path.join(test_result_path), exist_ok=True)
# model = tf.keras.models.load_model('../outputs/saved_model')
# print(model.summary())
# _, valid_ds = get_data()

# def evaluate(valid_ds):
#     # Get the results.
#     results = model.predict(valid_ds)
#     # Loop over the validation set and save the 
#     # images and corresponding result plot to disk.
#     counter = 0
#     for i, batch in tqdm(enumerate(valid_ds), total=len(valid_ds)):
#         for j, (image, keypoints) in enumerate(zip(batch[0], batch[1])):
#             evaluation_keypoints_plot(
#                 image, results[counter], keypoints,
#                 save_path=os.path.join(validation_result_path, str(counter)+'.png')
#             )
#             counter += 1

# def test(test_csv_path):

# #Function to predict on all images present in `test.csv` file

#     test_df = pd.read_csv(test_csv_path)
#     images = test_df.Image
#     for i in tqdm(range(len(images)), total=len(images)):
#         image = images.iloc[i].split(' ')
#         image = np.array(image, dtype=np.float32) / 255.
#         image = image.reshape(96, 96)
#         image = image.reshape(96, 96, 1)
#         image_batch = np.expand_dims(image, axis=0)
#         image_tensor = tf.convert_to_tensor(image_batch)
#         outputs = model.predict(image_tensor)
#         test_keypoints_plot(
#             image, outputs, 
#             save_path=os.path.join(test_result_path, str(i)+'.png')
#         )
# print('Evaluating...')
# evaluate(valid_ds)
# print('Testing...')
# test(test_csv_path=os.path.join('..', 'input', 'test.csv'))

# The next step is to configure the model:
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'acc'])
model.fit(x_train, y_train, batch_size=256, epochs=45, validation_split=0.2)
# A total number of 45 iterations were performed in the whole training dataset.

# To put the model to the test using our data collection, we must first prepare our test set.
test_df.isnull().any()

# x test: Separate and reshape input test values
image_df = test_df['Image']
keypoints_df = test_df.drop('Image', axis=1)
imageArr = []
for i in range(0, len(image_df)):
    img = image_df[i].split()
    img = ['0' if x=='' else x for x in img]
    imageArr.append(img)
x_test = np.array(imageArr, dtype='float')
x_test = x_test.reshape(-1, 96, 96, 1)
print(x_test.shape)

# We're now going to separate target values (y_test) in the test data set
y_test = np.array(keypoints_df, dtype='float')
print(y_test.shape)

# Now, it's time to predict the results of the trained model;
pred = model.predict(x_test)
idLookupTable.head()

